---
layout: post
title: 机器学习的泛化误差上界
subtitle: 
tags: MachineLearning
category: tech
---

小结一下 [Learning From Data](https://work.caltech.edu/lectures.html) Lecture 5~7 所学. 这是 Caltech 的一门机器学习基础课, 质量非常高. 我的课程笔记见 [Github](https://github.com/sunoonlee/machine-learning/blob/master/learning_from_data/Learning_From_Data_notes.md).

## 泛化

泛化能力是机器学习的关键. 我们利用数据来训练模型, 目的不是得到最小的训练误差, 而是让模型在一般情形下表现较好. 

为了衡量模型表现, 我们需要定义误差的度量 (Error Measure). 跟常说的损失函数/目标函数等名词也是一个意思. 我们按照 Learning From Data 里的方式定义三种误差:

- $E_{in}​$: in-sample error, 训练误差
- $E_{out}$: out-of-sample error, 训练集外的误差, 是在整个输入空间与输出空间上的误差期望.
- $E_{out} - E_{in}$: 泛化误差, 表示由训练集泛化至训练集外的过程中产生的误差. 不过, 也有人用"泛化误差"表示 $E_{out}$.

我们的目标是让 $E_{out}$ 尽量小, 这样模型在遇到新鲜数据时才能有好的表现, 这样的模型才是有用的. 但直接优化 $E_{out}$ 不现实, 而 $E_{in}$ 的优化相对容易, 那么在此基础上, 我们只需要使 $E_{out}$ 尽量接近 $E_{in}$ , 也就是尽量减小泛化误差.

泛化误差能否达到足够小, 决定了机器学习的可行性. 统计学习理论对这个问题给出了一些解答, 证明这种可行性在概率意义上成立.

## Hoeffding 不等式

先来回顾一下大数定律: 样本数量越多, 随机变量的样本均值就越接近期望值. 统计学习理论利用了大数定律的一种特殊形式: Hoeffding 不等式.

$$ P[|E_{out} - E_{in}| > \epsilon] \leq 2e^{-2\epsilon^2N}, \ \forall \epsilon > 0 $$

就是说, 当数据量 N 足够大时, 泛化误差与训练集误差有很大概率会非常接近. 听起来很美, 是不是泛化问题就这样解决了呢? 

可惜不然. 以上不等式只适用于事先确定的单一假设函数. 而在机器学习过程中, 最终的那个假设函数 g 是从一个比较大的集合里选出来的. 如果集合大小为 M, 对集合内不同的假设函数取 union bound, 那么不等式就变成了:

$$ P[|E_{out}(g) - E_{in}(g)| > \epsilon] \leq 2Me^{-2\epsilon^2N}, \ \forall \epsilon > 0 $$

多了一个 M, 结果就完全变味了. 要知道, 一般的机器学习模型,  M 都是无穷大, 因此不等式右边会很大, 作为一个概率的限值已经没有意义了.

## "有效"假设函数

幸运的是, 上面这个 M 取得很保守, 实际情况没有这么坏. 在无穷多的假设函数里, "有效"数量往往是有限的. 这种"有效性"表现在假设函数作用于数据集时的效果. 当多个假设函数在数据集上得到的分类结果相同时 (比如, 对三个数据点, 分类结果都是"正正负"), 有效数量为 1. (这样一个"有效假设函数" 叫做 dichotomy.)

(以下都是就二分类问题而言) 假设函数"有效"数量的上限是 $2^N$ , 达到这个上限时, 意味着假设函数集合 H 能够穷尽 N 个数据点分类的所有可能性 (这时可以说 H shatter 了数据点).

## VC 维

为了抽象出一个表达模型复杂度的量, V 同学和 C 同学一起提出了 VC 维 (VC dimension), 即: 一个假设函数集合 H 最多能 shatter 多少数据点.

以二维平面的感知机为例: 对三个不共线的点, 它总是能给出所有的分类可能, 但对四个点就办不到了. 因此二维感知机的 VC 维是 3. 更一般地, d 维感知机的 VC 维等于 d+1. VC 维可以看做对模型有效参数数量或自由度的一种度量.

## 泛化误差上界

借助"有效"假设函数和 VC 维的概念, 经过一些推导, 可以大幅减小前面不等式右端的概率限值. 选定一个 tolerance level $\delta$, 则下式以不小于 $1-\delta$ 的概率成立:

$$ E_{out} \leq E_{in} + \Omega(N,H,\delta) $$ 

其中, 
+ $ \Omega(N,H,\delta) = \sqrt{\frac{8}{N} ln\frac{4m_H(2N)}{\delta}}$.
+ $m_H(N)$ 是"增长函数", 表示任选 N 个点时, H 能得到的最大的 dichotomy 数量.

由于增长函数一般不太容易计算, 可以进一步用 VC 维表示它的上界: $m_H(N) \leq N^{d_{vc}} + 1$

于是, $\Omega(N,H,\delta) \leq \sqrt{\frac{8}{N} ln\frac{4((2N)^{d_{vc}} + 1)}{\delta}}$

以上就是历经千辛万苦得到的一个泛化误差上界, 这个结论非常宝贵, 因为它证明了机器学习在理论上的可行性. 不过, 因为这个上界具有广泛的一般性, 推导过程中多次放大, 最终得到的上界非常松, 实际应用的意义主要在其相对值而非绝对值.

可以看出影响泛化误差的两个因素: 数据量和模型复杂度. 数据量不必多说, 自然多多益善. 模型复杂度的影响更微妙一些: 当复杂度增加时, 一般 $E_{in}$ 会减小, 而泛化误差 $\Omega$ 项会增大; 为了得到最优的 $E_{out}$, 需要两者之间达到一种平衡.
